{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 1: Importaciones y Configuración Inicial del Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 14:38:17 - INFO - Agent GOSOM MVP Initialized (inicio de sesión de logger).\n",
      "2025-05-28 14:38:17 - INFO - [ OK ] Archivo de configuración 'c:\\Users\\El Bauto\\Documents\\MEGA\\Generative_ai\\etl-gmaps\\0_AGENTE_GOSOM\\config\\parameters_default.json' cargado correctamente.\n",
      "2025-05-28 14:38:17 - INFO - \n",
      "================================================================================\n",
      "CARGANDO PARÁMETROS GLOBALES DEL AGENTE\n",
      "================================================================================\n",
      "2025-05-28 14:38:17 - INFO - [INFO] Lenguaje para scraping: es\n",
      "2025-05-28 14:38:17 - INFO - [INFO] Profundidad de búsqueda por defecto: 2\n",
      "2025-05-28 14:38:17 - INFO - [INFO] Prefijo para archivos de resultados: gmaps_data_\n",
      "2025-05-28 14:38:17 - INFO - [INFO] Carpeta para CSVs crudos: c:\\Users\\El Bauto\\Documents\\MEGA\\Generative_ai\\etl-gmaps\\0_AGENTE_GOSOM\\data\\raw\n",
      "2025-05-28 14:38:17 - INFO - [INFO] Carpeta para CSVs procesados: c:\\Users\\El Bauto\\Documents\\MEGA\\Generative_ai\\etl-gmaps\\0_AGENTE_GOSOM\\data\\processed\n",
      "2025-05-28 14:38:17 - INFO - [ OK ] Carpetas de datos RAW y PROCESSED verificadas/creadas.\n",
      "2025-05-28 14:38:17 - INFO - [INFO] Coordenadas cargadas para 4 ciudades.\n",
      "2025-05-28 14:38:17 - INFO - [INFO] Celda 1 completada: Logger y configuración base listos.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG RUTA] Directorio del agente GOSOM asumido: c:\\Users\\El Bauto\\Documents\\MEGA\\Generative_ai\\etl-gmaps\\0_AGENTE_GOSOM\n",
      "[DEBUG RUTA] Intentando cargar config desde: c:\\Users\\El Bauto\\Documents\\MEGA\\Generative_ai\\etl-gmaps\\0_AGENTE_GOSOM\\config\\parameters_default.json\n",
      "\n",
      "        ********************************************************************************\n",
      "        *                     G M A P S   S C R A P E R   A G E N T                    *\n",
      "        *                         ( Avalian Project - MVP )                          *\n",
      "        ********************************************************************************\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Clase para Logger Estilizado (con caracteres ASCII simples para consola) ---\n",
    "class StyledLogger:\n",
    "    def __init__(self, logger_name='StyledLogger', log_file_path='agent.log', level=logging.INFO):\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.setLevel(level)\n",
    "        self.logger.propagate = False \n",
    "\n",
    "        if self.logger.hasHandlers():\n",
    "            self.logger.handlers.clear()\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # File Handler con UTF-8\n",
    "        try:\n",
    "            fh = logging.FileHandler(log_file_path, encoding='utf-8')\n",
    "            fh.setFormatter(formatter)\n",
    "            self.logger.addHandler(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al crear FileHandler para logs: {e}\")\n",
    "\n",
    "        # Stream Handler (consola del notebook)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setFormatter(formatter)\n",
    "        self.logger.addHandler(sh)\n",
    "\n",
    "        # Estilos ASCII simplificados para mayor compatibilidad de consola\n",
    "        self.HEADER_ART = \"\"\"\n",
    "        ********************************************************************************\n",
    "        *                     G M A P S   S C R A P E R   A G E N T                    *\n",
    "        *                         ( Avalian Project - MVP )                          *\n",
    "        ********************************************************************************\n",
    "        \"\"\"\n",
    "        self.SECTION_SEPARATOR = \"=\" * 80\n",
    "        self.SUB_SECTION_SEPARATOR = \"-\" * 60\n",
    "        self.SUCCESS_ART = \"[ OK ]\"\n",
    "        self.ERROR_ART = \"[FAIL]\"\n",
    "        self.WARNING_ART = \"[WARN]\"\n",
    "        self.INFO_ART = \"[INFO]\"\n",
    "        self.DEBUG_ART = \"[DEBUG]\"\n",
    "\n",
    "    def _log(self, level, message, art=\"\"):\n",
    "        self.logger.log(level, f\"{art} {message}\".strip())\n",
    "\n",
    "    def header(self):\n",
    "        # Imprimir arte ASCII grande directamente a la consola del notebook\n",
    "        # y un log formal al archivo de logs\n",
    "        print(self.HEADER_ART)\n",
    "        self.logger.info(\"Agent GOSOM MVP Initialized (inicio de sesión de logger).\")\n",
    "\n",
    "    def section(self, title):\n",
    "        self.logger.info(f\"\\n{self.SECTION_SEPARATOR}\\n{title.upper()}\\n{self.SECTION_SEPARATOR}\")\n",
    "\n",
    "    def subsection(self, title):\n",
    "        self.logger.info(f\"\\n{self.SUB_SECTION_SEPARATOR}\\n{title}\\n{self.SUB_SECTION_SEPARATOR}\")\n",
    "\n",
    "    def info(self, message):\n",
    "        self._log(logging.INFO, message, self.INFO_ART)\n",
    "\n",
    "    def success(self, message):\n",
    "        self._log(logging.INFO, message, self.SUCCESS_ART)\n",
    "\n",
    "    def warning(self, message):\n",
    "        self._log(logging.WARNING, message, self.WARNING_ART)\n",
    "\n",
    "    def error(self, message, exc_info=False):\n",
    "        self._log(logging.ERROR, message, self.ERROR_ART)\n",
    "        if exc_info: # Para capturar el traceback completo\n",
    "            self.logger.exception(\"Detalles de la excepción:\")\n",
    "\n",
    "    def critical(self, message, exc_info=False):\n",
    "        self._log(logging.CRITICAL, message, f\"{self.ERROR_ART} [CRITICAL]\")\n",
    "        if exc_info:\n",
    "            self.logger.exception(\"Detalles de la excepción crítica:\")\n",
    "            \n",
    "    def debug(self, message):\n",
    "        self._log(logging.DEBUG, message, self.DEBUG_ART)\n",
    "\n",
    "# --- Configuración de Rutas y Logger ---\n",
    "CONFIG_DIR = None\n",
    "DATA_DIR = None\n",
    "LOGS_DIR = None\n",
    "config_params = {}\n",
    "log_file_path_global = 'agent_gmaps_mvp_fallback.log'\n",
    "\n",
    "try:\n",
    "    # Asumimos que el notebook (.ipynb) está en la carpeta '0_AGENTE_GOSOM/notebooks/'\n",
    "    # y el CWD es esa carpeta 'notebooks/'\n",
    "    current_notebook_dir = os.getcwd()\n",
    "    print(f\"[DEBUG RUTA] Directorio actual del notebook (CWD): {current_notebook_dir}\")\n",
    "\n",
    "    # El directorio '0_AGENTE_GOSOM' es el padre de 'notebooks/'\n",
    "    agent_gosom_dir = os.path.dirname(current_notebook_dir)\n",
    "    print(f\"[DEBUG RUTA] Directorio del agente GOSOM deducido: {agent_gosom_dir}\")\n",
    "    \n",
    "    CONFIG_DIR = os.path.join(agent_gosom_dir, 'config')\n",
    "    DATA_DIR = os.path.join(agent_gosom_dir, 'data') # Carpeta 'data' dentro de '0_AGENTE_GOSOM'\n",
    "    LOGS_DIR = os.path.join(DATA_DIR, 'logs') # 'logs' dentro de 'data'\n",
    "\n",
    "    config_file_path = os.path.join(CONFIG_DIR, 'parameters_default.json')\n",
    "    print(f\"[DEBUG RUTA] Intentando cargar config desde: {config_file_path}\")\n",
    "\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        config_params = json.load(f)\n",
    "    \n",
    "    LOG_FILENAME = config_params.get('log_filename', 'agent_gmaps_mvp.log')\n",
    "    \n",
    "    os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "    log_file_path_global = os.path.join(LOGS_DIR, LOG_FILENAME)\n",
    "\n",
    "    logger = StyledLogger(log_file_path=log_file_path_global, level=logging.INFO) # o logging.DEBUG\n",
    "    logger.header()\n",
    "    logger.success(f\"Archivo de configuración '{config_file_path}' cargado correctamente.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Este print se verá incluso si el logger falla en instanciarse\n",
    "    print(f\"ERROR CRÍTICO [FileNotFoundError]: No se pudo encontrar el archivo de configuración en la ruta esperada: '{config_file_path}'. Por favor, verifica la estructura de carpetas y la ubicación del archivo 'parameters_default.json' en la carpeta 'config' relativa a '0_AGENTE_GOSOM'.\")\n",
    "    # Intentar instanciar logger con path de fallback para al menos loguear el error\n",
    "    logger = StyledLogger(log_file_path=log_file_path_global) \n",
    "    logger.critical(f\"Archivo de parámetros '{config_file_path}' NO ENCONTRADO. Usando defaults para logger y parámetros.\", exc_info=False) # No queremos exc_info si el problema es FileNotFoundError aquí\n",
    "except Exception as e:\n",
    "    print(f\"ERROR CRÍTICO al cargar configuración o inicializar logger: {e}\")\n",
    "    logger = StyledLogger(log_file_path=log_file_path_global)\n",
    "    logger.critical(f\"Error al cargar configuración o inicializar logger: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Carga de Parámetros Globales (Continuación) ---\n",
    "logger.section(\"Cargando Parámetros Globales del Agente\")\n",
    "\n",
    "LANGUAGE = config_params.get('language', 'es')\n",
    "logger.info(f\"Lenguaje para scraping: {LANGUAGE}\")\n",
    "\n",
    "DEFAULT_DEPTH = config_params.get('default_depth', 2)\n",
    "logger.info(f\"Profundidad de búsqueda por defecto: {DEFAULT_DEPTH}\")\n",
    "\n",
    "RESULTS_FILENAME_PREFIX = config_params.get('results_filename_prefix', 'gmaps_data_')\n",
    "logger.info(f\"Prefijo para archivos de resultados: {RESULTS_FILENAME_PREFIX}\")\n",
    "\n",
    "# Nombres de las subcarpetas de datos (del JSON)\n",
    "RAW_CSV_FOLDER_NAME = config_params.get('output_csv_folder_raw_name', 'raw')\n",
    "PROCESSED_CSV_FOLDER_NAME = config_params.get('output_csv_folder_processed_name', 'processed')\n",
    "\n",
    "# Rutas absolutas a las carpetas de datos (si DATA_DIR se definió correctamente)\n",
    "if DATA_DIR:\n",
    "    RAW_CSV_FOLDER = os.path.join(DATA_DIR, RAW_CSV_FOLDER_NAME)\n",
    "    PROCESSED_CSV_FOLDER = os.path.join(DATA_DIR, PROCESSED_CSV_FOLDER_NAME)\n",
    "\n",
    "    logger.info(f\"Carpeta para CSVs crudos: {RAW_CSV_FOLDER}\")\n",
    "    logger.info(f\"Carpeta para CSVs procesados: {PROCESSED_CSV_FOLDER}\")\n",
    "\n",
    "    # Crear carpetas de datos si no existen\n",
    "    try:\n",
    "        os.makedirs(RAW_CSV_FOLDER, exist_ok=True)\n",
    "        os.makedirs(PROCESSED_CSV_FOLDER, exist_ok=True)\n",
    "        logger.success(\"Carpetas de datos RAW y PROCESSED verificadas/creadas.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"No se pudieron crear las carpetas de datos '{RAW_CSV_FOLDER}' o '{PROCESSED_CSV_FOLDER}': {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.error(\"DATA_DIR no está definido. No se pueden establecer rutas para CSVs crudos/procesados.\")\n",
    "    RAW_CSV_FOLDER = None # Asegurar que estén definidos aunque sea como None\n",
    "    PROCESSED_CSV_FOLDER = None\n",
    "\n",
    "\n",
    "GMAPS_COORDINATES = config_params.get('gmaps_coordinates', {})\n",
    "if GMAPS_COORDINATES:\n",
    "    logger.info(f\"Coordenadas cargadas para {len(GMAPS_COORDINATES)} ciudades.\")\n",
    "else:\n",
    "    logger.warning(\"No se cargaron coordenadas de ciudades desde la configuración.\")\n",
    "\n",
    "logger.info(\"Celda 1 completada: Logger y configuración base listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 2 (Revisada): Funciones para Leer Archivos de Keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 14:38:33 - INFO - \n",
      "================================================================================\n",
      "DEFINIENDO FUNCIONES DE UTILIDAD\n",
      "================================================================================\n",
      "2025-05-28 14:38:33 - INFO - \n",
      "------------------------------------------------------------\n",
      "Función para Cargar Keywords\n",
      "------------------------------------------------------------\n",
      "2025-05-28 14:38:33 - INFO - \n",
      "------------------------------------------------------------\n",
      "Probando Carga de Keywords\n",
      "------------------------------------------------------------\n",
      "2025-05-28 14:38:33 - INFO - [INFO] Intentando cargar keywords para la ciudad de prueba: 'neuquen'\n",
      "2025-05-28 14:38:33 - INFO - [ OK ] Keywords cargadas para 'neuquen' desde 'c:\\Users\\El Bauto\\Documents\\MEGA\\Generative_ai\\etl-gmaps\\0_AGENTE_GOSOM\\config\\keywords_neuquen.csv': 4 keywords.\n",
      "2025-05-28 14:38:33 - INFO - [INFO] Ejemplo de keywords para Neuquen (primeras 3 si hay): ['Contadores en Neuquen', 'Empresas de servicios en Neuquen', 'Consultorios medicos en Neuquen']\n",
      "2025-05-28 14:38:33 - INFO - [INFO] Intentando cargar keywords para una ciudad de prueba que no existe: 'ciudad_totalmente_inventada'\n",
      "2025-05-28 14:38:33 - ERROR - [FAIL] Archivo de keywords no encontrado para 'ciudad_totalmente_inventada' en 'c:\\Users\\El Bauto\\Documents\\MEGA\\Generative_ai\\etl-gmaps\\0_AGENTE_GOSOM\\config\\keywords_ciudad_totalmente_inventada.csv'.\n",
      "2025-05-28 14:38:33 - INFO - [INFO] Celda 2 completada: Función de carga de keywords definida y probada.\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Funciones para Leer Archivos de Keywords\n",
    "\n",
    "logger.section(\"Definiendo Funciones de Utilidad\")\n",
    "logger.subsection(\"Función para Cargar Keywords\")\n",
    "\n",
    "def load_keywords_from_csv(city_name_key):\n",
    "    \"\"\"\n",
    "    Carga keywords desde un archivo CSV específico para una clave de ciudad.\n",
    "    La clave de ciudad se usa para encontrar el archivo en 'config/keywords_<city_name_key>.csv'\n",
    "    \"\"\"\n",
    "    if CONFIG_DIR is None:\n",
    "        logger.error(\"Directorio de configuración (CONFIG_DIR) no está definido. No se pueden cargar keywords.\")\n",
    "        return []\n",
    "\n",
    "    filepath = os.path.join(CONFIG_DIR, f'keywords_{city_name_key.lower()}.csv')\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            keywords = [line.strip() for line in f if line.strip()] # Filtra líneas vacías y quita espacios\n",
    "        \n",
    "        if keywords:\n",
    "            logger.success(f\"Keywords cargadas para '{city_name_key}' desde '{filepath}': {len(keywords)} keywords.\")\n",
    "        else:\n",
    "            logger.warning(f\"No se encontraron keywords válidas (o el archivo está vacío) en '{filepath}' para '{city_name_key}'.\")\n",
    "        return keywords\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Archivo de keywords no encontrado para '{city_name_key}' en '{filepath}'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar keywords para '{city_name_key}' desde '{filepath}': {e}\", exc_info=True)\n",
    "        return []\n",
    "\n",
    "# --- Pruebas de la función load_keywords_from_csv (Opcional) ---\n",
    "logger.subsection(\"Probando Carga de Keywords\")\n",
    "\n",
    "# Prueba con una ciudad que debería tener archivo de keywords\n",
    "test_city_existing = 'neuquen' # Asegúrate de tener '0_AGENTE_GOSOM/config/keywords_neuquen.csv'\n",
    "logger.info(f\"Intentando cargar keywords para la ciudad de prueba: '{test_city_existing}'\")\n",
    "keywords_test_existing = load_keywords_from_csv(test_city_existing)\n",
    "\n",
    "if keywords_test_existing:\n",
    "    logger.info(f\"Ejemplo de keywords para {test_city_existing.capitalize()} (primeras 3 si hay): {keywords_test_existing[:3]}\")\n",
    "else:\n",
    "    # El warning o error ya fue logueado por la función load_keywords_from_csv\n",
    "    pass\n",
    "\n",
    "# Prueba con una ciudad que NO debería tener archivo de keywords\n",
    "test_city_non_existing = 'ciudad_totalmente_inventada'\n",
    "logger.info(f\"Intentando cargar keywords para una ciudad de prueba que no existe: '{test_city_non_existing}'\")\n",
    "keywords_test_non_existing = load_keywords_from_csv(test_city_non_existing)\n",
    "# (Se espera un error en el log si el archivo no existe, y la función devuelve [])\n",
    "\n",
    "logger.info(\"Celda 2 completada: Función de carga de keywords definida y probada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 3 (Completa y Arreglada): Función para Ejecutar el Scraper GOSOM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Función para Ejecutar el Scraper GOSOM (usando Docker)\n",
    "\n",
    "logger.section(\"Definiendo Función para Ejecutar GOSOM Scraper vía Docker\")\n",
    "\n",
    "def run_gmaps_scraper_docker(keywords_list, city_name_key, depth_override=None):\n",
    "    \"\"\"\n",
    "    Ejecuta el scraper GOSOM usando Docker para una lista de keywords y una clave de ciudad.\n",
    "    La clave de ciudad se usa para obtener coordenadas y nombrar archivos.\n",
    "    Guarda los resultados en un archivo CSV en la carpeta RAW_CSV_FOLDER.\n",
    "    \"\"\"\n",
    "    logger.subsection(f\"Iniciando scraping para ciudad: '{city_name_key.capitalize()}'\")\n",
    "\n",
    "    if not keywords_list:\n",
    "        logger.warning(f\"No hay keywords para scrapear para '{city_name_key}'. Omitiendo.\")\n",
    "        return None\n",
    "\n",
    "    if not CONFIG_DIR or not RAW_CSV_FOLDER:\n",
    "        logger.error(\"CONFIG_DIR o RAW_CSV_FOLDER no están definidos. No se puede ejecutar el scraper.\")\n",
    "        return None\n",
    "\n",
    "    # Obtener coordenadas para la ciudad\n",
    "    city_info = GMAPS_COORDINATES.get(city_name_key.lower())\n",
    "    if not city_info:\n",
    "        logger.error(f\"No se encontraron coordenadas/info para '{city_name_key}' en la configuración (GMAPS_COORDINATES).\")\n",
    "        return None\n",
    "    \n",
    "    lat = city_info.get('latitude')\n",
    "    lon = city_info.get('longitude')\n",
    "    # radius = city_info.get('radius', 10000) # Radius es principalmente para fast-mode\n",
    "    # zoom = city_info.get('zoom', 14)      # Zoom es principalmente para fast-mode\n",
    "    \n",
    "    if lat is None or lon is None:\n",
    "        logger.error(f\"Latitud o longitud faltantes para '{city_name_key}'.\")\n",
    "        return None\n",
    "\n",
    "    current_depth = depth_override if depth_override is not None else DEFAULT_DEPTH\n",
    "    logger.info(f\"Usando profundidad de búsqueda: {current_depth}\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    raw_output_filename = f\"{RESULTS_FILENAME_PREFIX}{city_name_key.lower()}_{timestamp}.csv\"\n",
    "    raw_output_filepath_host = os.path.join(RAW_CSV_FOLDER, raw_output_filename)\n",
    "    raw_output_filepath_container = f\"/app/data/raw/{raw_output_filename}\" # Ruta DENTRO del contenedor\n",
    "\n",
    "    # Crear un archivo temporal para las queries en la carpeta config del HOST\n",
    "    temp_queries_filename = f\"temp_queries_{city_name_key.lower()}_{timestamp}.txt\"\n",
    "    temp_queries_filepath_host = os.path.join(CONFIG_DIR, temp_queries_filename)\n",
    "    temp_queries_filepath_container = f\"/app/config/{temp_queries_filename}\" # Ruta DENTRO del contenedor\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Creando archivo temporal de queries en: {temp_queries_filepath_host}\")\n",
    "        with open(temp_queries_filepath_host, 'w', encoding='utf-8') as f:\n",
    "            for kw in keywords_list:\n",
    "                f.write(f\"{kw}\\n\")\n",
    "        logger.success(f\"Archivo temporal de queries creado con {len(keywords_list)} keywords.\")\n",
    "\n",
    "        # GOSOM requiere que el archivo de resultados exista ANTES de ejecutarlo.\n",
    "        logger.info(f\"Creando archivo de salida CSV vacío en: {raw_output_filepath_host}\")\n",
    "        with open(raw_output_filepath_host, 'w', encoding='utf-8') as f:\n",
    "            pass # Simplemente crea el archivo vacío\n",
    "        logger.success(\"Archivo de salida CSV vacío creado.\")\n",
    "        \n",
    "        # Construcción del comando Docker\n",
    "        # Montamos la carpeta 'config' del host (que contiene temp_queries) a '/app/config' en el contenedor\n",
    "        # Montamos la carpeta 'data/raw' del host a '/app/data/raw' en el contenedor\n",
    "        docker_command = [\n",
    "            \"docker\", \"run\",\n",
    "            \"--rm\", \n",
    "            \"-v\", f\"{CONFIG_DIR}:/app/config:ro\",  # :ro para read-only si el contenedor no necesita escribir en config\n",
    "            \"-v\", f\"{RAW_CSV_FOLDER}:/app/data/raw\",\n",
    "            \"gosom/google-maps-scraper\",\n",
    "            \"-lang\", LANGUAGE,\n",
    "            \"-depth\", str(current_depth),\n",
    "            \"-input\", temp_queries_filepath_container,\n",
    "            \"-results\", raw_output_filepath_container,\n",
    "            \"-exit-on-inactivity\", \"3m\",\n",
    "            \"-geo\", f\"{lat},{lon}\"\n",
    "            # Si quieres activar emails (tarda mucho más):\n",
    "            # \"-email\"\n",
    "        ]\n",
    "\n",
    "        logger.info(f\"Ejecutando comando Docker: {' '.join(docker_command)}\")\n",
    "        \n",
    "        # Ejecutar el comando\n",
    "        process = subprocess.run(docker_command, capture_output=True, text=True, encoding='utf-8', check=False)\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            logger.success(f\"Scraping completado exitosamente para '{city_name_key}'.\")\n",
    "            logger.info(f\"Resultados guardados en: '{raw_output_filepath_host}'\")\n",
    "            if process.stdout:\n",
    "                logger.debug(f\"Salida estándar del scraper (stdout):\\n{process.stdout[:500]}...\") # Muestra solo una parte\n",
    "            if process.stderr:\n",
    "                logger.warning(f\"Salida de error estándar del scraper (stderr), si la hubo:\\n{process.stderr[:500]}...\")\n",
    "            return raw_output_filepath_host\n",
    "        else:\n",
    "            logger.error(f\"Error durante el scraping para '{city_name_key}'. Código de retorno: {process.returncode}\")\n",
    "            if process.stdout:\n",
    "                logger.error(f\"Salida estándar del scraper (stdout) en error:\\n{process.stdout}\")\n",
    "            if process.stderr:\n",
    "                logger.error(f\"Salida de error estándar del scraper (stderr) en error:\\n{process.stderr}\")\n",
    "            return None\n",
    "\n",
    "    except FileNotFoundError as fnf_error: # Específico para el comando docker si no se encuentra\n",
    "        logger.critical(f\"Comando 'docker' no encontrado. Asegúrate de que Docker esté instalado y en el PATH del sistema.\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Excepción inesperada al ejecutar el scraper para '{city_name_key}': {e}\", exc_info=True)\n",
    "        return None\n",
    "    finally:\n",
    "        # Eliminar el archivo temporal de queries\n",
    "        if os.path.exists(temp_queries_filepath_host):\n",
    "            try:\n",
    "                os.remove(temp_queries_filepath_host)\n",
    "                logger.info(f\"Archivo temporal de queries eliminado: {temp_queries_filepath_host}\")\n",
    "            except Exception as e_remove:\n",
    "                logger.warning(f\"No se pudo eliminar el archivo temporal de queries '{temp_queries_filepath_host}': {e_remove}\")\n",
    "\n",
    "# --- Prueba de la función run_gmaps_scraper_docker ---\n",
    "# Descomenta CON CUIDADO para probar una ejecución real. Asegúrate de que Docker Desktop esté corriendo.\n",
    "# Y que tengas 'keywords_neuquen.csv' en '0_AGENTE_GOSOM/config/'\n",
    "# Y que 'neuquen' esté definido en GMAPS_COORDINATES en 'parameters_default.json'\n",
    "\n",
    "\n",
    "'''\n",
    " logger.section(\"Probando Ejecución del Scraper GOSOM para Neuquén\")\n",
    " test_city_for_scraping = 'neuquen'\n",
    " keywords_for_scraping_test = load_keywords_from_csv(test_city_for_scraping)\n",
    "\n",
    " if keywords_for_scraping_test and GMAPS_COORDINATES.get(test_city_for_scraping.lower()):\n",
    "     logger.info(f\"Iniciando prueba de scraping real para: {test_city_for_scraping.capitalize()}\")\n",
    "     # Usar una profundidad muy baja para la prueba\n",
    "     results_file = run_gmaps_scraper_docker(keywords_for_scraping_test, test_city_for_scraping, depth_override=1) \n",
    "     if results_file:\n",
    "         logger.success(f\"PRUEBA DE SCRAPING FINALIZADA. Archivo generado: {results_file}\")\n",
    "         # Podrías añadir aquí una lectura rápida del CSV para ver si tiene datos\n",
    "         try:\n",
    "             df_test_results = pd.read_csv(results_file)\n",
    "             logger.info(f\"El archivo de resultados contiene {len(df_test_results)} filas.\")\n",
    "             if not df_test_results.empty:\n",
    "                 display(df_test_results.head(2))\n",
    "         except Exception as e_read:\n",
    "             logger.error(f\"No se pudo leer el archivo de resultados de prueba: {e_read}\")\n",
    "     else:\n",
    "         logger.error(f\"PRUEBA DE SCRAPING FALLÓ para {test_city_for_scraping.capitalize()}. Revisa los logs.\")\n",
    " else:\n",
    "     logger.warning(f\"No se ejecutará la prueba de scraping. Faltan keywords o coordenadas para '{test_city_for_scraping}'.\")\n",
    "'''\n",
    "\n",
    "\n",
    "logger.info(\"Celda 3 completada: Función para ejecutar GOSOM vía Docker definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 4: Orquestación del Proceso de Scraping para Ciudades Definidas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta celda utilizará las funciones de las celdas anteriores para iterar sobre una lista de ciudades (o todas las que tengan keywords y coordenadas), ejecutar el scraper para cada una, y recolectar las rutas a los archivos CSV crudos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Orquestación del Proceso de Scraping\n",
    "\n",
    "logger.section(\"Orquestando Tareas de Scraping\")\n",
    "\n",
    "# Lista de ciudades a procesar. Podrías obtenerlas de las claves en GMAPS_COORDINATES\n",
    "# o tener una lista explícita.\n",
    "# Por ahora, procesaremos solo las ciudades para las que tengamos archivos de keywords y coordenadas.\n",
    "\n",
    "cities_to_process = []\n",
    "if GMAPS_COORDINATES and CONFIG_DIR:\n",
    "    for city_key in GMAPS_COORDINATES.keys():\n",
    "        # Verificar si existe un archivo de keywords para esta ciudad\n",
    "        keywords_file_path = os.path.join(CONFIG_DIR, f'keywords_{city_key.lower()}.csv')\n",
    "        if os.path.exists(keywords_file_path):\n",
    "            cities_to_process.append(city_key)\n",
    "            logger.info(f\"Ciudad '{city_key.capitalize()}' añadida a la lista de procesamiento (tiene coordenadas y archivo de keywords).\")\n",
    "        else:\n",
    "            logger.warning(f\"No se encontró archivo de keywords para '{city_key}' en '{keywords_file_path}'. Se omitirá esta ciudad.\")\n",
    "else:\n",
    "    logger.error(\"No hay coordenadas (GMAPS_COORDINATES) o directorio de configuración (CONFIG_DIR) definido. No se pueden determinar ciudades a procesar.\")\n",
    "\n",
    "\n",
    "raw_csv_files_generated = {} # Diccionario para guardar: {'ciudad': 'ruta_al_csv_crudo'}\n",
    "\n",
    "if not cities_to_process:\n",
    "    logger.warning(\"No hay ciudades configuradas para procesar. Revisa GMAPS_COORDINATES y los archivos 'keywords_ciudad.csv'.\")\n",
    "else:\n",
    "    logger.info(f\"Ciudades a procesar en esta ejecución: {', '.join([c.capitalize() for c in cities_to_process])}\")\n",
    "    \n",
    "    for city_key in cities_to_process:\n",
    "        logger.subsection(f\"Procesando Ciudad: {city_key.capitalize()}\")\n",
    "        \n",
    "        keywords = load_keywords_from_csv(city_key)\n",
    "        if not keywords:\n",
    "            logger.warning(f\"No se cargaron keywords para {city_key.capitalize()}, se omite el scraping para esta ciudad.\")\n",
    "            continue\n",
    "\n",
    "        # Aquí puedes decidir la profundidad. Podrías tenerla en GMAPS_COORDINATES por ciudad,\n",
    "        # o usar un valor diferente para pruebas.\n",
    "        # depth_for_city = GMAPS_COORDINATES[city_key].get('depth', DEFAULT_DEPTH) \n",
    "        depth_for_city = 1 # Para el MVP y pruebas rápidas, usar una profundidad baja.\n",
    "        \n",
    "        logger.info(f\"Intentando scraping para {city_key.capitalize()} con {len(keywords)} keywords y profundidad {depth_for_city}.\")\n",
    "        \n",
    "        # --- ESTA LÍNEA EJECUTARÁ DOCKER ---\n",
    "        # (En la prueba real, esto puede tardar varios minutos por ciudad)\n",
    "        # raw_file_path = run_gmaps_scraper_docker(keywords, city_key, depth_override=depth_for_city)\n",
    "        # --- FIN DE LÍNEA QUE EJECUTA DOCKER ---\n",
    "\n",
    "        # --- PARA DESARROLLO SIN EJECUTAR DOCKER CADA VEZ ---\n",
    "        # Simula que el scraper se ejecutó y generó un archivo.\n",
    "        # Asegúrate de tener un archivo de ejemplo en data/raw/ para que esto funcione sin Docker.\n",
    "        # Por ejemplo: data/raw/gmaps_data_neuquen_dummy.csv\n",
    "        logger.warning(\"MODO SIMULACIÓN: `run_gmaps_scraper_docker` está COMENTADO. Usando archivo dummy si existe.\")\n",
    "        dummy_file_name = f\"{RESULTS_FILENAME_PREFIX}{city_key.lower()}_dummy.csv\"\n",
    "        dummy_file_path = os.path.join(RAW_CSV_FOLDER, dummy_file_name)\n",
    "        if os.path.exists(dummy_file_path):\n",
    "            raw_file_path = dummy_file_path\n",
    "            logger.info(f\"MODO SIMULACIÓN: Usando archivo dummy existente: {raw_file_path}\")\n",
    "        else:\n",
    "            raw_file_path = None\n",
    "            logger.warning(f\"MODO SIMULACIÓN: Archivo dummy NO encontrado en {dummy_file_path}. No habrá datos para esta ciudad en simulación.\")\n",
    "        # --- FIN DE SECCIÓN DE SIMULACIÓN ---\n",
    "\n",
    "        if raw_file_path and os.path.exists(raw_file_path):\n",
    "            raw_csv_files_generated[city_key] = raw_file_path\n",
    "            logger.success(f\"Scraping para {city_key.capitalize()} simulado/completado. Archivo: {raw_file_path}\")\n",
    "        else:\n",
    "            logger.error(f\"Scraping para {city_key.capitalize()} falló o no generó archivo (o archivo dummy no encontrado).\")\n",
    "\n",
    "if raw_csv_files_generated:\n",
    "    logger.success(f\"Proceso de scraping (o simulación) finalizado. Archivos crudos generados:\")\n",
    "    for city, path in raw_csv_files_generated.items():\n",
    "        logger.info(f\"  - {city.capitalize()}: {path}\")\n",
    "else:\n",
    "    logger.warning(\"No se generaron archivos CSV crudos en esta ejecución.\")\n",
    "\n",
    "logger.info(\"Celda 4 completada: Orquestación de scraping (o simulación) definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 5: Carga y Transformación de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Carga y Transformación de Datos\n",
    "\n",
    "logger.section(\"Cargando y Transformando Datos Extraídos\")\n",
    "\n",
    "# Nombres de las columnas esperadas del CSV de GOSOM\n",
    "# ¡IMPORTANTE! Asegúrate de que el orden y los nombres coincidan con la salida real de GOSOM.\n",
    "# Esta lista debe ser igual a la que usaste en tu EDA.\n",
    "gmaps_column_names = [\n",
    "    'input_id', 'link', 'title', 'category', 'address', 'open_hours', 'popular_times',\n",
    "    'website', 'phone', 'plus_code', 'review_count', 'review_rating', 'reviews_per_rating',\n",
    "    'latitude', 'longitude', 'cid', 'status', 'descriptions', 'reviews_link', 'thumbnail',\n",
    "    'timezone', 'price_range', 'data_id', 'images', 'reservations', 'order_online',\n",
    "    'menu', 'owner', 'complete_address', 'about', 'user_reviews', 'emails'\n",
    "    # Añadir 'user_reviews_extended' si se usa el flag --extra-reviews\n",
    "]\n",
    "\n",
    "# Lista para almacenar todos los DataFrames procesados de cada ciudad\n",
    "all_processed_data = []\n",
    "\n",
    "# Función de transformación (puedes expandirla mucho más)\n",
    "def transform_gmaps_data(df_raw, city_key_origin):\n",
    "    logger.subsection(f\"Transformando datos para: {city_key_origin.capitalize()}\")\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # 1. Convertir tipos de datos\n",
    "    logger.info(\"Convirtiendo tipos de datos numéricos...\")\n",
    "    for col in ['review_count', 'review_rating', 'latitude', 'longitude']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        else:\n",
    "            logger.warning(f\"Columna '{col}' no encontrada para conversión numérica.\")\n",
    "            df[col] = pd.NA # Añadirla como NA si no existe para evitar errores posteriores\n",
    "\n",
    "    # 2. Parsear 'complete_address' para extraer componentes\n",
    "    # (Reutilizando la función del EDA, asegúrate de que esté definida si no la copiaste aquí)\n",
    "    def extract_address_components(json_str_or_data):\n",
    "        try:\n",
    "            if pd.isna(json_str_or_data): return pd.Series([None, None, None, None, None], index=['parsed_street', 'parsed_city_comp', 'parsed_postal_code', 'parsed_state', 'parsed_country'])\n",
    "            \n",
    "            data = json.loads(str(json_str_or_data)) if isinstance(json_str_or_data, str) else json_str_or_data\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                return pd.Series([\n",
    "                    data.get('street'), data.get('city'), data.get('postal_code'), \n",
    "                    data.get('state'), data.get('country')\n",
    "                ], index=['parsed_street', 'parsed_city_comp', 'parsed_postal_code', 'parsed_state', 'parsed_country'])\n",
    "            # Añadir manejo si 'data' es una lista, si es necesario basado en tu EDA\n",
    "            return pd.Series([None, None, None, None, None], index=['parsed_street', 'parsed_city_comp', 'parsed_postal_code', 'parsed_state', 'parsed_country'])\n",
    "        except: # Captura amplia para evitar que falle la transformación\n",
    "            return pd.Series([None, None, None, None, None], index=['parsed_street', 'parsed_city_comp', 'parsed_postal_code', 'parsed_state', 'parsed_country'])\n",
    "\n",
    "    if 'complete_address' in df.columns:\n",
    "        logger.info(\"Parseando 'complete_address'...\")\n",
    "        address_components = df['complete_address'].apply(extract_address_components)\n",
    "        df = pd.concat([df, address_components], axis=1)\n",
    "        logger.success(\"Componentes de dirección parseados y añadidos.\")\n",
    "    else:\n",
    "        logger.warning(\"Columna 'complete_address' no encontrada. No se parsearán componentes de dirección.\")\n",
    "\n",
    "\n",
    "    # 3. Añadir columna de origen (la ciudad de la búsqueda)\n",
    "    df['search_origin_city'] = city_key_origin.capitalize()\n",
    "    logger.info(f\"Añadida columna 'search_origin_city' con valor '{city_key_origin.capitalize()}'.\")\n",
    "\n",
    "    # 4. Selección de columnas relevantes para Avalian (ejemplo)\n",
    "    # Ajusta esta lista según las necesidades reales de tu BD y CRM\n",
    "    relevant_columns = [\n",
    "        'title', 'category', 'address', 'parsed_street', 'parsed_city_comp', 'parsed_postal_code', 'parsed_state', \n",
    "        'website', 'phone', 'review_count', 'review_rating', 'latitude', 'longitude', \n",
    "        'link', 'search_origin_city' # 'link' es el enlace a Gmaps, 'search_origin_city' es la que añadimos\n",
    "        # Considera también: 'cid', 'status', 'open_hours'\n",
    "    ]\n",
    "    # Filtrar para mantener solo columnas que realmente existen en el df después de transformaciones\n",
    "    final_columns = [col for col in relevant_columns if col in df.columns]\n",
    "    if len(final_columns) < len(relevant_columns):\n",
    "        missing_for_selection = set(relevant_columns) - set(final_columns)\n",
    "        logger.warning(f\"Algunas columnas relevantes para selección no existen en el DataFrame: {missing_for_selection}\")\n",
    "    \n",
    "    df_processed = df[final_columns].copy() # Usar .copy() para evitar SettingWithCopyWarning\n",
    "    logger.success(f\"Seleccionadas {len(df_processed.columns)} columnas relevantes.\")\n",
    "    \n",
    "    # 5. Limpieza adicional (ejemplos)\n",
    "    if 'phone' in df_processed.columns:\n",
    "        df_processed.loc[:, 'phone'] = df_processed['phone'].str.replace(r'[^\\d\\+\\s]', '', regex=True).str.strip() # Limpiar caracteres no deseados del teléfono\n",
    "    \n",
    "    logger.info(f\"Transformación completada para {city_key_origin.capitalize()}. {len(df_processed)} registros procesados.\")\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "if not raw_csv_files_generated:\n",
    "    logger.warning(\"No hay archivos CSV crudos para procesar (variable 'raw_csv_files_generated' está vacía).\")\n",
    "else:\n",
    "    for city_key, raw_file_path in raw_csv_files_generated.items():\n",
    "        logger.subsection(f\"Procesando archivo crudo para: {city_key.capitalize()} ({raw_file_path})\")\n",
    "        try:\n",
    "            # Leer el CSV crudo. GOSOM no escribe encabezado.\n",
    "            df_raw_city = pd.read_csv(raw_file_path, header=None, names=gmaps_column_names, on_bad_lines='skip')\n",
    "            logger.info(f\"Leído archivo CSV crudo para {city_key.capitalize()}. {len(df_raw_city)} filas encontradas.\")\n",
    "            \n",
    "            if not df_raw_city.empty:\n",
    "                df_processed_city = transform_gmaps_data(df_raw_city, city_key)\n",
    "                all_processed_data.append(df_processed_city)\n",
    "                logger.success(f\"Datos de {city_key.capitalize()} transformados y añadidos a la lista general.\")\n",
    "            else:\n",
    "                logger.warning(f\"El archivo CSV para {city_key.capitalize()} estaba vacío. No se procesaron datos.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logger.warning(f\"El archivo CSV para {city_key.capitalize()} en '{raw_file_path}' está vacío o es inválido.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al procesar el archivo para {city_key.capitalize()} ({raw_file_path}): {e}\", exc_info=True)\n",
    "\n",
    "if all_processed_data:\n",
    "    # Combinar todos los DataFrames procesados en uno solo\n",
    "    final_df_all_cities = pd.concat(all_processed_data, ignore_index=True)\n",
    "    logger.success(f\"Todos los datos procesados combinados. Total de {len(final_df_all_cities)} prospectos.\")\n",
    "    logger.info(\"Muestra de los primeros 5 prospectos combinados:\")\n",
    "    display(final_df_all_cities.head())\n",
    "else:\n",
    "    logger.warning(\"No se procesaron datos de ninguna ciudad. El DataFrame final está vacío.\")\n",
    "    final_df_all_cities = pd.DataFrame() # Crear un DF vacío para que el resto del notebook no falle\n",
    "\n",
    "logger.info(\"Celda 5 completada: Carga y transformación de datos definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de deduplicación (añadir después de concatenar en Celda 5)\n",
    "if not final_df_all_cities.empty:\n",
    "    logger.subsection(\"Aplicando Deduplicación de Prospectos\")\n",
    "    # Deduplicar basado en el link de Google Maps, manteniendo la primera aparición\n",
    "    # Podrías necesitar una lógica más sofisticada si los links no son siempre perfectos o si quieres fusionar info\n",
    "    initial_count = len(final_df_all_cities)\n",
    "    final_df_all_cities.drop_duplicates(subset=['link'], keep='first', inplace=True)\n",
    "    deduplicated_count = initial_count - len(final_df_all_cities)\n",
    "    if deduplicated_count > 0:\n",
    "        logger.success(f\"Se eliminaron {deduplicated_count} prospectos duplicados basados en la columna 'link'.\")\n",
    "    else:\n",
    "        logger.info(\"No se encontraron duplicados basados en la columna 'link' o ya estaban limpios.\")\n",
    "    logger.info(f\"Total de prospectos únicos después de deduplicación: {len(final_df_all_cities)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 6: Guardado de Datos Procesados (a un único CSV por ahora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6: Guardado de Datos Procesados\n",
    "\n",
    "logger.section(\"Guardando Datos Procesados y Limpios\")\n",
    "\n",
    "if not final_df_all_cities.empty:\n",
    "    timestamp_save = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    processed_filename = f\"gmaps_prospectos_consolidados_{timestamp_save}.csv\"\n",
    "    \n",
    "    if PROCESSED_CSV_FOLDER: # Verificar que la carpeta de destino esté definida\n",
    "        processed_filepath = os.path.join(PROCESSED_CSV_FOLDER, processed_filename)\n",
    "        try:\n",
    "            final_df_all_cities.to_csv(processed_filepath, index=False, encoding='utf-8-sig') # utf-8-sig para mejor compatibilidad con Excel\n",
    "            logger.success(f\"Datos procesados consolidados guardados en: {processed_filepath}\")\n",
    "            logger.info(f\"Total de {len(final_df_all_cities)} prospectos guardados.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al guardar el archivo CSV procesado en '{processed_filepath}': {e}\", exc_info=True)\n",
    "    else:\n",
    "        logger.error(\"PROCESSED_CSV_FOLDER no está definido. No se puede guardar el archivo procesado.\")\n",
    "elif raw_csv_files_generated and not all_processed_data : # Hubo archivos crudos pero no se procesó nada\n",
    "    logger.warning(\"Se generaron archivos crudos pero no se pudieron procesar o resultaron vacíos. No se guardará archivo consolidado.\")\n",
    "else: # No hubo archivos crudos para empezar\n",
    "    logger.warning(\"No hay datos procesados para guardar (DataFrame final está vacío).\")\n",
    "\n",
    "logger.info(\"Celda 6 completada: Proceso de guardado de datos procesados definido.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
